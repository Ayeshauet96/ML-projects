{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Knn(X_train, Y_train, X_test, k):\n",
    "    '''\n",
    "    Performs K-nearest neighbours classification on the test instances by looking for k minimum distances for \n",
    "    each instance from the training data. \n",
    "    Arguments\n",
    "    ---------\n",
    "    X_train : 2d-array\n",
    "        The train dataset of shape (m, n), where m is the number of training examples and n is the number of features.\n",
    "    Y_train : array\n",
    "        The labels of train data.\n",
    "    X_test : 2d-array\n",
    "        The test dataset of shape (p, n), where p is the number of test instances and n is the number of features.\n",
    "    k : int\n",
    "      The number of nearest neighbours to be used.\n",
    "    Returns\n",
    "    -------\n",
    "    Y_test :  array\n",
    "        The predicted instances of test data.\n",
    "    \n",
    "    '''\n",
    "    p = X_test.shape[0]\n",
    "    Y_test = []\n",
    "    for i in range(0, p):\n",
    "        distance = np.sqrt(np.sum(np.square(X_train - X_test[i, :]), axis=1)) # euclidean distance\n",
    "        k_neigh_index = np.argsort(distance)[0:k]    # select k minimum distances\n",
    "        k_neigh = Y_train[k_neigh_index]     # find labels of the minimum distances \n",
    "        k_neigh_count = Counter(k_neigh)   \n",
    "        Y_test.append(k_neigh_count.most_common(1)[0][0])      # select the most common label\n",
    "    return np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual_values, predicted_values):\n",
    "    '''\n",
    "    Generates confusion matrix for classification evaluation.\n",
    "    Arguments\n",
    "    ---------\n",
    "    actual_values : array\n",
    "        The actual decoded labels of test data: 1 for \"Iris-setosa\", 2 for \"Iris-versicolor\" and 3 for \"Iris-virginica\".\n",
    "        \n",
    "    predicted_values : array\n",
    "        The predicted decoded labels of test data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    conf_matrix : 2d array\n",
    "        The confusion matrix\n",
    "    \n",
    "    '''\n",
    "    conf_matrix = np.zeros((3,3))\n",
    "    for i in range(0, len(actual_values)):\n",
    "        if actual_values[i] == predicted_values[i]:\n",
    "            if actual_values[i] == 1:\n",
    "                conf_matrix[0,0] = conf_matrix[0,0] + 1\n",
    "            if actual_values[i] == 2:\n",
    "                conf_matrix[1,1] = conf_matrix[1,1] + 1\n",
    "            if actual_values[i] == 3:\n",
    "                conf_matrix[2,2] = conf_matrix[2,2] + 1\n",
    "        else:\n",
    "            if actual_values[i] == 1 and predicted_values[i] == 2:\n",
    "                conf_matrix[1,0] = conf_matrix[1,0] + 1\n",
    "            if actual_values[i] == 1 and predicted_values[i] == 3:\n",
    "                conf_matrix[2,0] = conf_matrix[2,0] + 1\n",
    "            if actual_values[i] == 2 and predicted_values[i] == 1:\n",
    "                conf_matrix[0,1] = conf_matrix[0,1] + 1\n",
    "            if actual_values[i] == 2 and predicted_values[i] == 3:\n",
    "                conf_matrix[2,1] = conf_matrix[2,1] + 1\n",
    "            if actual_values[i] == 3 and predicted_values[i] == 1:\n",
    "                conf_matrix[0,2] = conf_matrix[0,2] + 1\n",
    "            if actual_values[i] == 3 and predicted_values[i] == 2:\n",
    "                conf_matrix[1,2] = conf_matrix[1,2] + 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(conf_matrix):\n",
    "    '''\n",
    "    Generates micro and macro average scores for classification evaluation using the confusion matrix.\n",
    "    Arguments\n",
    "    ---------\n",
    "    conf_matrix : 2d array\n",
    "        The confusion matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    classification_report : dataframe\n",
    "        A dataframe containing micro and macro average precision, recall, accuracy and F1-scores.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    tp_c1 = conf_matrix[0,0]\n",
    "    fp_c1 = conf_matrix[0,1] + conf_matrix[0,2]\n",
    "    fn_c1 = conf_matrix[1,0] + conf_matrix[2,0]\n",
    "    tn_c1 = conf_matrix[1,1] + conf_matrix[1,2] + conf_matrix[2,1] + conf_matrix[2,2]\n",
    "    precision_c1 = tp_c1 / (tp_c1 + fp_c1)\n",
    "    recall_c1 = tp_c1 / (tp_c1 + fn_c1)\n",
    "    acc_c1 = (tp_c1 + tn_c1) / (tp_c1 + fp_c1 + tn_c1 + fn_c1)\n",
    "    F1_score_c1 = (2 * precision_c1 * recall_c1) / (precision_c1 + recall_c1)\n",
    "    \n",
    "    tp_c2 = conf_matrix[1,1]\n",
    "    fp_c2 = conf_matrix[1,0] + conf_matrix[1,2]\n",
    "    fn_c2 = conf_matrix[0,1] + conf_matrix[2,1]\n",
    "    tn_c2 = conf_matrix[0,0] + conf_matrix[0,2] + conf_matrix[2,0] + conf_matrix[2,2]\n",
    "    precision_c2 = tp_c2 / (tp_c2 + fp_c2)\n",
    "    recall_c2 = tp_c2 / (tp_c2 + fn_c2)\n",
    "    acc_c2 = (tp_c2 + tn_c2) / (tp_c2 + fp_c2 + tn_c2 + fn_c2)\n",
    "    F1_score_c2 = (2 * precision_c2 * recall_c2) / (precision_c2 + recall_c2)\n",
    "    \n",
    "    tp_c3 = conf_matrix[2,2]\n",
    "    fp_c3 = conf_matrix[2,0] + conf_matrix[2,1]\n",
    "    fn_c3 = conf_matrix[0,2] + conf_matrix[1,2]\n",
    "    tn_c3 = conf_matrix[0,0] + conf_matrix[0,1] + conf_matrix[1,0] + conf_matrix[1,1]\n",
    "    precision_c3 = tp_c3 / (tp_c3 + fp_c3)\n",
    "    recall_c3 = tp_c3 / (tp_c3 + fn_c3)\n",
    "    acc_c3 = (tp_c3 + tn_c3) / (tp_c3 + fp_c3 + tn_c3 + fn_c3)\n",
    "    F1_score_c3 = (2 * precision_c3 * recall_c3) / (precision_c3 + recall_c3)\n",
    "    \n",
    "    \n",
    "    macro_prec = (precision_c1 + precision_c2 + precision_c3) / 3\n",
    "    macro_recall = (recall_c1 + recall_c2 + recall_c3) / 3\n",
    "    macro_acc = (acc_c1 + acc_c2 + acc_c3) / 3\n",
    "    macro_F1 = (F1_score_c1 + F1_score_c2 + F1_score_c3) / 3\n",
    "    \n",
    "    micro_prec = (tp_c1 + tp_c2 + tp_c3) / (tp_c1 + tp_c2 + tp_c3 + fp_c1 + fp_c2 + fp_c3)\n",
    "    micro_recall = (tp_c1 + tp_c2 + tp_c3) / (tp_c1 + tp_c2 + tp_c3 + fn_c1 + fn_c2 + fn_c3)\n",
    "    micro_acc = (tp_c1 + tp_c2 + tp_c3 + tn_c1 + tn_c2 + tn_c3) / (tp_c1 + tp_c2 + tp_c3 + tn_c1 + tn_c2 + tn_c3 + fp_c1 + fp_c2 + fp_c3 + fn_c1 + fn_c2 + fn_c3)\n",
    "    micro_F1 = (2 * micro_prec * micro_recall) / (micro_prec + micro_recall)\n",
    "    \n",
    "    data = np.array([[macro_prec, macro_recall, macro_acc, macro_F1], [micro_prec, micro_recall, micro_acc, micro_F1]])\n",
    "    classification_report = pd.DataFrame(data, columns = ['precision', 'recall', 'accuracy', 'F1_score'], index=['macro_average', 'micro_avg'])\n",
    "    return classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding String Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(Y):\n",
    "    '''\n",
    "    This function decodes the labels.\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : array\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        size m, where m is the number of training examples.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y_d : list\n",
    "          Decoded values . 1 for \"Iris-setosa\" class, 2 for \"Iris-versicolor\" and 3 for \"Iris-virginica\".\n",
    "\n",
    "    '''\n",
    "    Y_d = []\n",
    "    for i in range(0, len(Y)):\n",
    "        if Y[i] == 'Iris-setosa':\n",
    "            Y_d.append(1)\n",
    "        if Y[i] == 'Iris-versicolor':\n",
    "            Y_d.append(2)\n",
    "        if Y[i] == 'Iris-virginica':\n",
    "            Y_d.append(3)\n",
    "    return Y_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', header=None)\n",
    "df_test = pd.read_csv('test.csv', header=None)\n",
    "X_train = np.array(df_train.drop(4, axis=1))\n",
    "Y_train = np.array(df_train[4])\n",
    "X_test = np.array(df_test.drop(4, axis=1))\n",
    "Y_test = np.array(df_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = decode(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat1 = Knn(X_train, Y_train, X_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat1 = decode(Y_hat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix1 = confusion_matrix(Y_test, Y_hat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0.],\n",
       "       [0., 5., 0.],\n",
       "       [0., 0., 5.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_report1 = classification_report(conf_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>macro_average</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro_avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision  recall  accuracy  F1_score\n",
       "macro_average        1.0     1.0       1.0       1.0\n",
       "micro_avg            1.0     1.0       1.0       1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_report1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat2 = Knn(X_train, Y_train, X_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat2 = decode(Y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix2 = confusion_matrix(Y_test, Y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0.],\n",
       "       [0., 5., 0.],\n",
       "       [0., 0., 5.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_report2 = classification_report(conf_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>macro_average</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro_avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision  recall  accuracy  F1_score\n",
       "macro_average        1.0     1.0       1.0       1.0\n",
       "micro_avg            1.0     1.0       1.0       1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_report2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat3 = Knn(X_train, Y_train, X_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat3 = decode(Y_hat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix3 = confusion_matrix(Y_test, Y_hat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0.],\n",
       "       [0., 5., 0.],\n",
       "       [0., 0., 5.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_report3 = classification_report(conf_matrix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>macro_average</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro_avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision  recall  accuracy  F1_score\n",
       "macro_average        1.0     1.0       1.0       1.0\n",
       "micro_avg            1.0     1.0       1.0       1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_report3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
