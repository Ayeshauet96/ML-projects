{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --user gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = list(csv.reader(open('./train.txt', encoding=\"utf8\")))\n",
    "test_data  = list(csv.reader(open('./test.txt', encoding=\"utf8\")))\n",
    "stop_words  = list(csv.reader(open('./stopwords.txt', encoding=\"utf8\")))\n",
    "stop_words = [row[0] for row in  stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    text_to_return = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            text_to_return = text_to_return + char\n",
    "    return text_to_return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    tokens= text.split()\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method cleanSingleNews Takes a single news text and cleans it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSingleNews(text):\n",
    "    text = removeStopWords(text)\n",
    "    text = removePunctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(data):\n",
    "    data =  [ [row[0], cleanSingleNews(row[1])]  for row in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned = cleanData(train_data)\n",
    "test_data_cleaned = cleanData(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this statement takes a while to load, just holup\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News headline as a Average Vector \n",
    "_getSingleSentenceFeatures()_\n",
    "- Ignores the word(s) those are unknown to model\n",
    "- If all words in sentence are unknown, then return vector with all zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSingleSentenceFeatures(sentence):\n",
    "    words = sentence.split()\n",
    "    \n",
    "    #check and remove if any word is not in models vocab\n",
    "    words_filtered = [word for word in words if word in model.vocab ]\n",
    "    \n",
    "    #words removed from sentence\n",
    "    '''\n",
    "    print('Following Words Ignored from the sentence:')\n",
    "    for word in words:\n",
    "        if word not in words_filtered:\n",
    "            print(word , end = ' ')\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if len(words_filtered) == 0: #rare case: if all words are unknown to model\n",
    "        return np.zeros(300)\n",
    "\n",
    "    features = model[words_filtered]\n",
    "    #print((features.shape))\n",
    "    avg_features = np.mean(features, axis=0)\n",
    "    return avg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(data):\n",
    "    features_M =  [ [data_row[0], getSingleSentenceFeatures(data_row[1])] for data_row in data]\n",
    "    return features_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = buildFeatures(train_data_cleaned)\n",
    "test_features = buildFeatures(test_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStats(algoResults):\n",
    "    \n",
    "    print('\\nAlgorithm\\t|TPs\\t|TNs\\t|FPs\\t|FNs\\t\\t|F1-Score\\t|Accuracy\\t|Precision\\t|Recall','\\n'+'================================================================================================================')\n",
    "    for algoResults_i in algoResults:\n",
    "        confusionMatrix = algoResults_i[1]\n",
    "        algoName = algoResults_i[0]\n",
    "        \n",
    "        tp = int(confusionMatrix[0][0])\n",
    "        fp = int(confusionMatrix[0][1])\n",
    "        fn = int(confusionMatrix[1][0])\n",
    "        tn = int(confusionMatrix[1][1])\n",
    "\n",
    "\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "        f1Score = 2*((precision * recall) / (precision + recall))\n",
    "        \n",
    "        dec_fig = 3\n",
    "        precision= round(precision, dec_fig)\n",
    "        recall= round(recall, dec_fig)\n",
    "        accuracy= round(accuracy,dec_fig )\n",
    "        f1Score= round(f1Score, dec_fig)\n",
    "        \n",
    "        print(algoName+'\\t|'+str(tp)+'\\t|'+str(tn)+'\\t|'+str(fp)+'\\t|'+str(fn)+'\\t\\t|'+str(f1Score)+'   \\t|'+str(accuracy)+'   \\t|'+str(precision)+'   \\t|'+str(recall))\n",
    "        #print('------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic function for three algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatix(actualLabels, predictedLabels):\n",
    "    confusionMatrix = np.zeros([2,2])\n",
    "    for index, label in enumerate(actualLabels):\n",
    "        goldLabel  = int(label)\n",
    "        predictedLabel = int(predictedLabels[index])\n",
    "        #if not goldLabel == predictedLabel:\n",
    "        #print('goldLabel', goldLabel, \"predictedLabel\",predictedLabel,'@',index)\n",
    "        if goldLabel == predictedLabel:\n",
    "            #print(\"Correct Prediction\")\n",
    "            if(goldLabel):\n",
    "                confusionMatrix[0][0] = confusionMatrix[0][0] +1\n",
    "                #print(\"TP for\")\n",
    "            else:\n",
    "                confusionMatrix[1][1] = confusionMatrix[1][1] +1\n",
    "                #print(\"TN for\")\n",
    "        else:\n",
    "            if(goldLabel):\n",
    "                confusionMatrix[1][0] = confusionMatrix[1][0] +1\n",
    "            else:\n",
    "                confusionMatrix[0][1] = confusionMatrix[0][1] +1\n",
    "\n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCosts(data, y_label, title):\n",
    "    pyplot.plot(data, color='b')\n",
    "    pyplot.xlabel('Iterations')\n",
    "    pyplot.ylabel(y_label)\n",
    "    pyplot.title(title)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pyplot.gcf())\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, theta1, theta2):\n",
    "    '''\n",
    "    Function to calculate softmax of three hypotheses.\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : array\n",
    "        A training example of shape n.\n",
    "        \n",
    "    theta1 : array\n",
    "        The weights vector of shape (n+1,) for class 1.\n",
    "        \n",
    "    theta2 : array\n",
    "        The weights vector of shape (n+1,) for class 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    h_x : list\n",
    "          A list containing the softmax of hypothesis of each class.\n",
    "\n",
    "    '''\n",
    "    h_x = list()\n",
    "    x = np.append(1,x)\n",
    "    z1 = sum(theta1 * x)\n",
    "    z2 = sum(theta2 * x)\n",
    "    \n",
    "\n",
    "    Z = [z1, z2]\n",
    "    Z_exp = np.exp(Z)\n",
    "    h_x = Z_exp/np.sum(Z_exp)\n",
    "    '''\n",
    "    \n",
    "    sum_exp = np.exp(z1) + np.exp(z2) \n",
    "    h1_x = np.exp(z1) / sum_exp\n",
    "    h2_x = np.exp(z2) / sum_exp\n",
    "    \n",
    "    \n",
    "   \n",
    "    h_x = [h1_x, h2_x]\n",
    "    \n",
    "    #print(h_x, h_x_m)\n",
    "    '''\n",
    "    return h_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(X, Y, theta1, theta2):\n",
    "    '''\n",
    "    This function calculates the cross-entropy-loss for multinomial logistic regression.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        The input dataset of shape (m, n), where m is the number of training examples and n is the number of features.\n",
    "    \n",
    "    Y : array\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        shape (m, k), where m is the number of training examples and k is the number of categories.\n",
    "    \n",
    "    theta1 : array\n",
    "        The weights vector of shape (n+1,) for class 1.\n",
    "        \n",
    "    theta2 : array\n",
    "        The weights vector of shape (n+1,) for class 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : array\n",
    "        The value of multinomial the logistic regression cost function for each training example.\n",
    "\n",
    "    '''\n",
    "    # initialize some useful values\n",
    "    m = X.shape[0] \n",
    "    n = X.shape[1] #number of features\n",
    "    k = Y.shape[1]\n",
    "    # You need to return the following variable(s) correctly\n",
    "    J = 0\n",
    " \n",
    " \n",
    "    \n",
    "    hx = np.array([softmax(X[i,:], theta1, theta2) for i in range(0, m)])\n",
    "    \n",
    "    for i in range(0, k):\n",
    "         J = J + ((-1/m)*sum(Y[:,i] * np.log(np.array(hx[:,i]))))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X,X_with_X01, Y, alpha, n_epoch, batch_size):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn thetas. Updates thetas for each class simultaneously,  by taking `n_epoch`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        The input dataset of shape (m, n), where m is the number of training examples and n is the number of features.\n",
    "    \n",
    "    X_with_X01 : 2d-array\n",
    "        The input dataset of shape (m+1, n), where m is the number of training examples and n is the number of features. Appending 1 to handle bias (X0 = 1)\n",
    "    \n",
    "    Y : array\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        shape (m, k), where m is the number of training examples and k is the number of categories.\n",
    "        \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    n_epoch : int\n",
    "        The number of iterations for gradient descent. \n",
    "        \n",
    "    batch_size :  int\n",
    "        Size of the batch for Gradient Descent\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta1 : array\n",
    "        The weights vector of shape (n+1,) for class 1.\n",
    "        \n",
    "    theta2 : array\n",
    "        The weights vector of shape (n+1,) for class 2.\n",
    "        \n",
    "        \n",
    "    J : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize some useful values\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    n = X.shape[1]\n",
    "    J = list()  # list to store cost\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    #, dtype=np.longdouble\n",
    "    \n",
    "    theta1 = np.zeros(n+1)\n",
    "    theta2 = np.zeros(n+1)\n",
    "    \n",
    "    #X_with_X01 = np.concatenate((np.ones((m,1)),X), axis=1)\n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        percent  = float(\"{0:.1f}\".format((epoch/n_epoch)*100)) \n",
    "        ### START CODE HERE ### (≈ 5-10 lines of code)\n",
    "        hx = np.array([softmax(X[i,:], theta1, theta2) for i in range(0, m)])\n",
    "        for j in range(0, m, batch_size):\n",
    "            sys.stdout.write(\"\\rLogistic Regression: Epoch => \"+str(percent)+\"%\"+\"(\"+str(epoch+1)+\"/\"+str(n_epoch)+\") \\t Batches(size=\"+str(batch_size)+\"): \"+str(int(j/batch_size))+\" / \"+str(int(m/batch_size)))\n",
    "            sys.stdout.flush()\n",
    "            for k in range(0, n+1):\n",
    "                temp1 = (alpha/batch_size) * sum((hx[j:j+batch_size,0]-Y[j:j+batch_size,0]) * X_with_X01[j:j+batch_size,k] )\n",
    "                theta1[k] = theta1[k] - temp1\n",
    "                \n",
    "                temp2 = (alpha/batch_size) * sum((hx[j:j+batch_size,1]-Y[j:j+batch_size,1]) * X_with_X01[j:j+batch_size,k] )\n",
    "                theta2[k] = theta2[k] - temp2\n",
    "                \n",
    "        ### END CODE HERE ###\n",
    "        sys.stdout.write('\\rCalculating Cost for current epoch........') #do not change this\n",
    "        J.append(cross_entropy_loss(X, Y, theta1, theta2))\n",
    "#         if(epoch>2):\n",
    "#             plotCosts(J, 'Cost', 'Logistic Regression Cost')\n",
    "    return theta1, theta2, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logReg(X, theta1, theta2):\n",
    "    '''\n",
    "    Function which selects the most probable class by taking the max of probabilities.\n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        The test dataset of shape (m, n), where m is the number of instances and n is the number of features.\n",
    "    \n",
    "    theta1 : array\n",
    "        The weights vector of shape (n+1,) for class 1.\n",
    "        \n",
    "    theta2 : array\n",
    "        The weights vector of shape (n+1,) for class 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : array\n",
    "        The predicted values of the function at each data point. This is a vector of\n",
    "        shape (m, k), where m is the number of instances and k is the number of categories.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    Y_predict = list()\n",
    "    for i in range(0, m):\n",
    "        h_x = softmax(X[i, :], theta1, theta2)\n",
    "        max_arg = np.argmax(np.array(h_x))\n",
    "        #print('max_arg',max_arg, h_x)\n",
    "        if max_arg == 0:\n",
    "            Y_predict.append([1, 0])\n",
    "        else:\n",
    "            Y_predict.append([0, 1])\n",
    "    Y_predict = np.array(Y_predict)\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLogisticRegression(train_features, alpha,n_epoch, batch_size):\n",
    "\n",
    "    Y_train = np.array(train_features)[:,0]\n",
    "    X_train = np.array(train_features)[:,1]\n",
    "    X_train = np.array([x for x in X_train])\n",
    "    X_with_X01 = np.append(np.ones((len(X_train),1)), X_train , axis=1)\n",
    "    Y = []\n",
    "    for label_i in Y_train:\n",
    "        if label_i == '0':\n",
    "            Y.append([1,0])\n",
    "        else: \n",
    "            Y.append([0,1])\n",
    "\n",
    "    Y = np.array([y for y in Y])\n",
    "    \n",
    "    start = time.time()\n",
    "    theta1, theta2, J1 = batch_gradient_descent(X_train, X_with_X01 ,Y, alpha,n_epoch, batch_size)\n",
    "    end = time.time()\n",
    "    print(\"\\nLogistic Regression Training Completed in \", str(float(\"{0:.2f}\".format((end-start)/60)))+\" minute(s)\")\n",
    "    #print('Predicted theta = {}, cost = {}' .format (theta1, J1[-1]))\n",
    "    return theta1, theta2, J1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosineSimilarity(vector_train, vector_test):\n",
    "    if(len(vector_train)!=len(vector_test)):\n",
    "        print('Invalid Arguments')\n",
    "        return 0\n",
    "    denominator = (np.linalg.norm(vector_train)*np.linalg.norm(vector_test))\n",
    "    if denominator == 0:\n",
    "        #print(\"Returning 0 for cosine similarity\")\n",
    "        return 0\n",
    "    cosine_similarity = np.dot(vector_train, vector_test)/denominator\n",
    "    #print(cosine_similarity)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarities_knn(train_data, test_data_i):\n",
    "    similarities = []\n",
    "    vector_test = test_data_i[1] \n",
    "    label_test = test_data_i[0]\n",
    "   \n",
    "    for train_data_i in train_data:\n",
    "        vector_train = train_data_i[1]\n",
    "        label_train = train_data_i[0]\n",
    "        cosine_similarity = getCosineSimilarity(vector_train, vector_test)\n",
    "        #print(distance)\n",
    "        similarities.append([cosine_similarity, label_train])\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decideLabel_knn(knns , K):\n",
    "    knns= np.array(knns)\n",
    "    #print('knns',knns)\n",
    "    all_labels = knns[:,1]\n",
    "    #print(\"all_labels\", all_labels, knns)\n",
    "    mode_result = stats.mode(all_labels) \n",
    "    mode = mode_result[0] #mode at index zero \n",
    "    label = mode[0] #mode is returned as array\n",
    "    return label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKNNs(train_data, test_data_i, K):\n",
    "    #print(test_data_i) \n",
    "    similarities = getSimilarities_knn(train_data, test_data_i)\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[0])\n",
    "    \n",
    "    knn = similarities[-K:]#distances[0:K]#\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn(train_data, test_data_i, K):\n",
    "    #print(test_data_i)\n",
    "    k_neighbours = getKNNs(train_data, test_data_i, K)\n",
    "    #print(\"KNNs\",np.array(k_neighbours))\n",
    "    label = decideLabel_knn(k_neighbours,K)\n",
    "    #print(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(Y):\n",
    "    '''\n",
    "    Function changes labels from 0 and 1 to -1 and 1.\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : array\n",
    "        array with labels having values 0 and 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : array\n",
    "        array with labels changed to 1 and -1.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    for i in range(0, len(Y)):\n",
    "        Y[i] = int(Y[i])\n",
    "        if Y[i] == 0:\n",
    "            Y[i]= -1;\n",
    "    return Y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_learning_algorithm(X, y, epoch):\n",
    "    '''\n",
    "    Function implements the perceptron learning algorithm. It updates weights when y(wx)<0.\n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        Input data of shape (m,n)\n",
    "    y : array\n",
    "        output labels of shape m.\n",
    "    epoch : int\n",
    "        number of times the perceptron learning algorithm runs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array\n",
    "        learned weights for perceptron learning algorithm.\n",
    "\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    w = np.zeros(n) \n",
    "    errors = []\n",
    "    for ep in range(0, epoch):\n",
    "        percent  = float(\"{0:.1f}\".format((ep/epoch)*100)) \n",
    "        error = 0\n",
    "        for i in range(0, m):\n",
    "            product = y[i]*sum(X[i, :]*w)\n",
    "            if product <= 0:\n",
    "                w = w + (y[i] * X[i, :])\n",
    "                error = error + 1\n",
    "        errors.append(error)\n",
    "        sys.stdout.write(\"\\rPerceptron Learning Epoch => \"+str(percent)+\"%\"+\"(\"+str(ep+1)+\"/\"+str(epoch)+\")\"+\" \\tErrors=\"+str(error))\n",
    "        sys.stdout.flush()\n",
    "#         if(ep>2 and ep%10==0):\n",
    "#                plotCosts(errors,'Errors','Perceptron Weights Learning')\n",
    "        if error == 0:\n",
    "            break\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_perceptron(X, w):\n",
    "    '''\n",
    "    predicts the output of perceptron learning algorithm.\n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        Input data of shape (m,n)\n",
    "    w : array\n",
    "        learned weights for perceptron learning algorithm.\n",
    "    Returns\n",
    "    -------\n",
    "    y_predict : array\n",
    "        predicted outputs for perceptron learning algorithm.\n",
    "\n",
    "    '''\n",
    "    #print(w)\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    y_predict = []\n",
    "    for i in range(0, m):\n",
    "        product = sum(X[i, :]*w)\n",
    "        if product < 0:\n",
    "            y_predict.append(-1)\n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "    y_predict = np.array(y_predict)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(y):\n",
    "    '''\n",
    "    changes the output labels back to 0 and 1 from -1 and 1.\n",
    "    Arguments\n",
    "    ---------\n",
    "    y : array\n",
    "        array with labels having values -1 and 1.\n",
    "    Returns\n",
    "    -------\n",
    "    y : array\n",
    "        array with labels having values 0 and 1.\n",
    "\n",
    "    '''\n",
    "    y_new = np.zeros(len(y))\n",
    "    for i in range(0, len(y)):\n",
    "        if y[i]==-1:\n",
    "            y_new[i]=0\n",
    "        else:\n",
    "            y_new[i]=1\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executePerceptron(train_features, test_features, n_epoch):\n",
    "    '''\n",
    "    This function trains and tests the perceptron learning algorithm.\n",
    "    Arguments\n",
    "    ---------\n",
    "    train_features : 2d-array\n",
    "        array contains traing \n",
    "    train_features : 2d-array\n",
    "        array with labels having values -1 and 1.\n",
    "    train_features : 2d-array\n",
    "        array with labels having values -1 and 1.\n",
    "    Returns\n",
    "    -------\n",
    "    confusionMatrix : array\n",
    "        array with labels having values 0 and 1.\n",
    "    '''\n",
    "    X_train = np.array([list(train_features[i][1]) for i in range(0, len(train_features))])\n",
    "    Y_train = np.array([int(train_features[i][0]) for i in range(0, len(train_features))])\n",
    "    Y_train = change_labels(Y_train)\n",
    "    X_test = np.array([list(test_features[i][1]) for i in range(0, len(test_features))])\n",
    "    Y_test = np.array([int(test_features[i][0]) for i in range(0, len(test_features))])\n",
    "    Y_test = change_labels(Y_test)\n",
    "    weights, errors = perceptron_learning_algorithm(X_train, Y_train, n_epoch)\n",
    "    Y_hat = predict_perceptron(X_test, weights)\n",
    "    \n",
    "    actualLabels = decode_labels(Y_test)\n",
    "    predictedLabels = decode_labels(Y_hat)\n",
    "    confusionMatrix = getConfusionMatix(actualLabels, predictedLabels)\n",
    "    \n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeKNN(train_features, test_features, K):\n",
    "    #logic goes here\n",
    "    \n",
    "    \n",
    "    t_featuers = test_features[0:300]\n",
    "    \n",
    "    actualLabels = np.array(t_featuers)[:,0]\n",
    "    #print(actualLabels)\n",
    "    predictedLabels = []\n",
    "    testDataSize= len(t_featuers)\n",
    "   \n",
    "    for index, test_features_i in enumerate(t_featuers):\n",
    "        percent  = float(\"{0:.1f}\".format((index/testDataSize)*100)) \n",
    "        sys.stdout.write(\"\\r\"+str(K)+\"-NN Tests =>\\t \"+str(percent)+\"%\"+\"(\"+str(index+1)+\"/\"+str(testDataSize)+\"Samples)\")\n",
    "        sys.stdout.flush()\n",
    "        label = classify_knn(train_features, test_features_i, K)\n",
    "        predictedLabels.append(label)\n",
    "    #print(actualLabels, predictedLabels)\n",
    "    confusionMatrix = getConfusionMatix(actualLabels, predictedLabels)\n",
    "    return confusionMatrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeLogisticRegression(train_features, test_features, alpha,n_epoch, batch_size):\n",
    "    theta1, theta2, J1 = trainLogisticRegression(train_features, alpha,n_epoch, batch_size)\n",
    "    \n",
    "    X_test = np.array(test_features)[:,1]\n",
    "    X_test = np.array([x for x in X_test])\n",
    "    X_test = np.array([x for x in X_test])\n",
    "    actualLabels = np.array(test_features)[:,0]\n",
    "    predictedLabels  = predict_logReg(X_test, theta1, theta2)\n",
    "    predictedLabels = [0 if pl[0]==1 else 1 for pl in predictedLabels]\n",
    "    confusionMatrix = getConfusionMatix(actualLabels, predictedLabels)\n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function executes 3 algorithm \n",
    "\n",
    "- Logistic Regression with Settings:\n",
    " -- Alpha = 0.001\n",
    " -- Iterations = 200\n",
    " -- Batch Size = 32\n",
    " \n",
    " \n",
    " - Kth Nearest Neighbour with: \n",
    " -- K={1,3, 5, 7, 10}\n",
    " \n",
    " \n",
    " - Perceptron with:\n",
    " -- Iterations=500\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareResults(train_features, test_features):\n",
    "    \n",
    "    algoResults = []\n",
    "    \n",
    "    alpha = 0.001\n",
    "    n_epoch= 50\n",
    "    batch_size =32\n",
    "    confusionMatrix_lr = executeLogisticRegression(train_features, test_features,alpha,n_epoch, batch_size )\n",
    "    algoResults.append(['LogisticReg', confusionMatrix_lr])\n",
    "    \n",
    "    \n",
    "    \n",
    "    Ks=[1, 3, 5, 7, 10]\n",
    "    for K in Ks:\n",
    "        confusionMatrix_knn = executeKNN(train_features, test_features, K)\n",
    "        print('\\n')#printing aline intentionally\n",
    "        algoResults.append([str(K)+'-NN\\t', confusionMatrix_knn])\n",
    "    \n",
    "    \n",
    "    n_epoch_perceptron = 500\n",
    "    confusionMatrix_perceptron = executePerceptron(train_features, test_features, n_epoch_perceptron)\n",
    "    algoResults.append(['Perceptron', confusionMatrix_perceptron])\n",
    "    \n",
    "    return algoResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Cost for current epoch........ \t Batches(size=32): 688 / 688\n",
      "Logistic Regression Training Completed in  18.44 minute(s)\n",
      "1-NN Tests =>\t 99.7%(300/300Samples)\n",
      "\n",
      "3-NN Tests =>\t 99.7%(300/300Samples)\n",
      "\n",
      "5-NN Tests =>\t 99.7%(300/300Samples)\n",
      "\n",
      "7-NN Tests =>\t 99.7%(300/300Samples)\n",
      "\n",
      "10-NN Tests =>\t 99.7%(300/300Samples)\n",
      "\n",
      "Perceptron Learning Epoch => 99.8%(500/500) \tErrors=7837"
     ]
    }
   ],
   "source": [
    "algoResults = compareResults(train_features, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algorithm\t|TPs\t|TNs\t|FPs\t|FNs\t\t|F1-Score\t|Accuracy\t|Precision\t|Recall \n",
      "================================================================================================================\n",
      "LogisticReg\t|1606\t|2264\t|733\t|1121\t\t|0.634   \t|0.676   \t|0.687   \t|0.589\n",
      "1-NN\t\t|91\t|122\t|42\t|45\t\t|0.677   \t|0.71   \t|0.684   \t|0.669\n",
      "3-NN\t\t|102\t|121\t|43\t|34\t\t|0.726   \t|0.743   \t|0.703   \t|0.75\n",
      "5-NN\t\t|104\t|121\t|43\t|32\t\t|0.735   \t|0.75   \t|0.707   \t|0.765\n",
      "7-NN\t\t|107\t|125\t|39\t|29\t\t|0.759   \t|0.773   \t|0.733   \t|0.787\n",
      "10-NN\t\t|93\t|138\t|26\t|43\t\t|0.729   \t|0.77   \t|0.782   \t|0.684\n",
      "Perceptron\t|2351\t|1211\t|1786\t|376\t\t|0.685   \t|0.622   \t|0.568   \t|0.862\n"
     ]
    }
   ],
   "source": [
    "printStats(algoResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
